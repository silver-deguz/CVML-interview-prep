{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Sn0mfUIn8ji"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpU1PduBvX7F",
        "outputId": "d0eb5ed0-3621-4b2d-e6fe-f4c979171a42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 loss = 26026.4765625\n",
            "epoch 1 loss = 145.384765625\n",
            "epoch 2 loss = 144.22402954101562\n",
            "epoch 3 loss = 143.07252502441406\n",
            "epoch 4 loss = 141.93026733398438\n",
            "epoch 5 loss = 140.79708862304688\n",
            "epoch 6 loss = 139.67294311523438\n",
            "epoch 7 loss = 138.55780029296875\n",
            "epoch 8 loss = 137.45156860351562\n",
            "epoch 9 loss = 136.35415649414062\n",
            "epoch 10 loss = 135.26547241210938\n",
            "epoch 11 loss = 134.18551635742188\n",
            "epoch 12 loss = 133.1141815185547\n",
            "epoch 13 loss = 132.05140686035156\n",
            "epoch 14 loss = 130.99710083007812\n",
            "epoch 15 loss = 129.9512176513672\n",
            "epoch 16 loss = 128.91371154785156\n",
            "epoch 17 loss = 127.88446044921875\n",
            "epoch 18 loss = 126.86341857910156\n",
            "epoch 19 loss = 125.8505630493164\n",
            "epoch 20 loss = 124.84574890136719\n",
            "epoch 21 loss = 123.8489761352539\n",
            "epoch 22 loss = 122.86018371582031\n",
            "epoch 23 loss = 121.8792495727539\n",
            "epoch 24 loss = 120.90618896484375\n",
            "epoch 25 loss = 119.94087219238281\n",
            "epoch 26 loss = 118.98323822021484\n",
            "epoch 27 loss = 118.03327941894531\n",
            "epoch 28 loss = 117.0909194946289\n",
            "epoch 29 loss = 116.15605163574219\n",
            "epoch 30 loss = 115.22867584228516\n",
            "epoch 31 loss = 114.30867767333984\n",
            "epoch 32 loss = 113.39604187011719\n",
            "epoch 33 loss = 112.49068450927734\n",
            "epoch 34 loss = 111.59256744384766\n",
            "epoch 35 loss = 110.70159912109375\n",
            "epoch 36 loss = 109.81776428222656\n",
            "epoch 37 loss = 108.94097900390625\n",
            "epoch 38 loss = 108.07118225097656\n",
            "epoch 39 loss = 107.2083511352539\n",
            "epoch 40 loss = 106.35240173339844\n",
            "epoch 41 loss = 105.5032730102539\n",
            "epoch 42 loss = 104.66094970703125\n",
            "epoch 43 loss = 103.8253402709961\n",
            "epoch 44 loss = 102.99639892578125\n",
            "epoch 45 loss = 102.174072265625\n",
            "epoch 46 loss = 101.35830688476562\n",
            "epoch 47 loss = 100.54906463623047\n",
            "epoch 48 loss = 99.74629211425781\n",
            "epoch 49 loss = 98.94991302490234\n",
            "epoch 50 loss = 98.15989685058594\n",
            "epoch 51 loss = 97.3761978149414\n",
            "epoch 52 loss = 96.59873962402344\n",
            "epoch 53 loss = 95.8274917602539\n",
            "epoch 54 loss = 95.0624008178711\n",
            "epoch 55 loss = 94.30342864990234\n",
            "epoch 56 loss = 93.5505142211914\n",
            "epoch 57 loss = 92.80360412597656\n",
            "epoch 58 loss = 92.06266021728516\n",
            "epoch 59 loss = 91.32762908935547\n",
            "epoch 60 loss = 90.59847259521484\n",
            "epoch 61 loss = 89.87512969970703\n",
            "epoch 62 loss = 89.15756225585938\n",
            "epoch 63 loss = 88.44573211669922\n",
            "epoch 64 loss = 87.73958587646484\n",
            "epoch 65 loss = 87.03907775878906\n",
            "epoch 66 loss = 86.34415435791016\n",
            "epoch 67 loss = 85.65477752685547\n",
            "epoch 68 loss = 84.97091674804688\n",
            "epoch 69 loss = 84.2925033569336\n",
            "epoch 70 loss = 83.61951446533203\n",
            "epoch 71 loss = 82.95189666748047\n",
            "epoch 72 loss = 82.28960418701172\n",
            "epoch 73 loss = 81.63261413574219\n",
            "epoch 74 loss = 80.98086547851562\n",
            "epoch 75 loss = 80.33431243896484\n",
            "epoch 76 loss = 79.69291687011719\n",
            "epoch 77 loss = 79.05665588378906\n",
            "epoch 78 loss = 78.42546081542969\n",
            "epoch 79 loss = 77.79930114746094\n",
            "epoch 80 loss = 77.17816925048828\n",
            "epoch 81 loss = 76.56196594238281\n",
            "epoch 82 loss = 75.95069885253906\n",
            "epoch 83 loss = 75.34431457519531\n",
            "epoch 84 loss = 74.7427749633789\n",
            "epoch 85 loss = 74.1460189819336\n",
            "epoch 86 loss = 73.55403137207031\n",
            "epoch 87 loss = 72.96678161621094\n",
            "epoch 88 loss = 72.38420104980469\n",
            "epoch 89 loss = 71.80628967285156\n",
            "epoch 90 loss = 71.23299407958984\n",
            "epoch 91 loss = 70.66426849365234\n",
            "epoch 92 loss = 70.10008239746094\n",
            "epoch 93 loss = 69.5404052734375\n",
            "epoch 94 loss = 68.98519134521484\n",
            "epoch 95 loss = 68.43441772460938\n",
            "epoch 96 loss = 67.88803100585938\n",
            "epoch 97 loss = 67.34601593017578\n",
            "epoch 98 loss = 66.80833435058594\n",
            "epoch 99 loss = 66.27491760253906\n",
            "epoch 100 loss = 65.74578857421875\n",
            "epoch 101 loss = 65.22087097167969\n",
            "epoch 102 loss = 64.70015716552734\n",
            "epoch 103 loss = 64.18357849121094\n",
            "epoch 104 loss = 63.67115020751953\n",
            "epoch 105 loss = 63.16279220581055\n",
            "epoch 106 loss = 62.65851593017578\n",
            "epoch 107 loss = 62.15824508666992\n",
            "epoch 108 loss = 61.6619758605957\n",
            "epoch 109 loss = 61.169654846191406\n",
            "epoch 110 loss = 60.6812744140625\n",
            "epoch 111 loss = 60.19679641723633\n",
            "epoch 112 loss = 59.71619415283203\n",
            "epoch 113 loss = 59.23942184448242\n",
            "epoch 114 loss = 58.766456604003906\n",
            "epoch 115 loss = 58.297264099121094\n",
            "epoch 116 loss = 57.831817626953125\n",
            "epoch 117 loss = 57.370094299316406\n",
            "epoch 118 loss = 56.91204833984375\n",
            "epoch 119 loss = 56.45766067504883\n",
            "epoch 120 loss = 56.00690460205078\n",
            "epoch 121 loss = 55.55974578857422\n",
            "epoch 122 loss = 55.11616134643555\n",
            "epoch 123 loss = 54.676109313964844\n",
            "epoch 124 loss = 54.23957443237305\n",
            "epoch 125 loss = 53.8065299987793\n",
            "epoch 126 loss = 53.37693405151367\n",
            "epoch 127 loss = 52.950775146484375\n",
            "epoch 128 loss = 52.52801513671875\n",
            "epoch 129 loss = 52.10863494873047\n",
            "epoch 130 loss = 51.692596435546875\n",
            "epoch 131 loss = 51.27988815307617\n",
            "epoch 132 loss = 50.87046432495117\n",
            "epoch 133 loss = 50.464317321777344\n",
            "epoch 134 loss = 50.06140899658203\n",
            "epoch 135 loss = 49.661720275878906\n",
            "epoch 136 loss = 49.265220642089844\n",
            "epoch 137 loss = 48.871891021728516\n",
            "epoch 138 loss = 48.4817008972168\n",
            "epoch 139 loss = 48.0946159362793\n",
            "epoch 140 loss = 47.71063232421875\n",
            "epoch 141 loss = 47.329708099365234\n",
            "epoch 142 loss = 46.95182418823242\n",
            "epoch 143 loss = 46.576961517333984\n",
            "epoch 144 loss = 46.2050895690918\n",
            "epoch 145 loss = 45.83618927001953\n",
            "epoch 146 loss = 45.47023391723633\n",
            "epoch 147 loss = 45.107200622558594\n",
            "epoch 148 loss = 44.747066497802734\n",
            "epoch 149 loss = 44.389801025390625\n",
            "epoch 150 loss = 44.03539276123047\n",
            "epoch 151 loss = 43.68381118774414\n",
            "epoch 152 loss = 43.33504104614258\n",
            "epoch 153 loss = 42.98905563354492\n",
            "epoch 154 loss = 42.645835876464844\n",
            "epoch 155 loss = 42.30534744262695\n",
            "epoch 156 loss = 41.96758270263672\n",
            "epoch 157 loss = 41.632511138916016\n",
            "epoch 158 loss = 41.30011749267578\n",
            "epoch 159 loss = 40.97038269042969\n",
            "epoch 160 loss = 40.643272399902344\n",
            "epoch 161 loss = 40.31877899169922\n",
            "epoch 162 loss = 39.99687194824219\n",
            "epoch 163 loss = 39.67753982543945\n",
            "epoch 164 loss = 39.360755920410156\n",
            "epoch 165 loss = 39.04649353027344\n",
            "epoch 166 loss = 38.73474884033203\n",
            "epoch 167 loss = 38.42548751831055\n",
            "epoch 168 loss = 38.11869812011719\n",
            "epoch 169 loss = 37.814361572265625\n",
            "epoch 170 loss = 37.51244354248047\n",
            "epoch 171 loss = 37.212947845458984\n",
            "epoch 172 loss = 36.91584014892578\n",
            "epoch 173 loss = 36.6211051940918\n",
            "epoch 174 loss = 36.3287239074707\n",
            "epoch 175 loss = 36.03867721557617\n",
            "epoch 176 loss = 35.75094223022461\n",
            "epoch 177 loss = 35.46550750732422\n",
            "epoch 178 loss = 35.182350158691406\n",
            "epoch 179 loss = 34.901451110839844\n",
            "epoch 180 loss = 34.622802734375\n",
            "epoch 181 loss = 34.346378326416016\n",
            "epoch 182 loss = 34.07215118408203\n",
            "epoch 183 loss = 33.80012512207031\n",
            "epoch 184 loss = 33.5302619934082\n",
            "epoch 185 loss = 33.2625617980957\n",
            "epoch 186 loss = 32.99699020385742\n",
            "epoch 187 loss = 32.733543395996094\n",
            "epoch 188 loss = 32.47219467163086\n",
            "epoch 189 loss = 32.21294021606445\n",
            "epoch 190 loss = 31.955745697021484\n",
            "epoch 191 loss = 31.700613021850586\n",
            "epoch 192 loss = 31.447513580322266\n",
            "epoch 193 loss = 31.196435928344727\n",
            "epoch 194 loss = 30.947362899780273\n",
            "epoch 195 loss = 30.700275421142578\n",
            "epoch 196 loss = 30.455169677734375\n",
            "epoch 197 loss = 30.21201515197754\n",
            "epoch 198 loss = 29.97080421447754\n",
            "epoch 199 loss = 29.731515884399414\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "creates a simple NN that will predict the square root of a input\n",
        "'''\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear0 = nn.Linear(5,5)\n",
        "    self.linear1 = nn.Linear(5,5)\n",
        "\n",
        "    torch.nn.init.xavier_normal_(self.linear0.weight)\n",
        "    torch.nn.init.xavier_normal_(self.linear1.weight)\n",
        "\n",
        "    # net = nn.Sequential(self.linear0, self.linear1)\n",
        "    self.net = nn.Sequential(self.linear0, nn.ReLU(), self.linear1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "def train(net, inputs, targets, n_epochs, learning_rate):\n",
        "  square_loss = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "    outputs = net(inputs)\n",
        "    # loss = square_loss(targets**2, inputs)\n",
        "    # loss = square_loss(targets, inputs**0.5)\n",
        "    loss = square_loss(outputs, targets)\n",
        "\n",
        "    net.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'epoch {i} loss = {loss}')\n",
        "    \n",
        "\n",
        "torch.manual_seed(331)\n",
        "\n",
        "net = MLP()\n",
        "inputs = torch.Tensor([[4, 9, 16, 81, 625]])\n",
        "targets = torch.Tensor([[2, 3, 4, 9, 25]])\n",
        "n_epochs = 200\n",
        "learning_rate = 1e-2\n",
        "\n",
        "train(net, inputs, targets, n_epochs, learning_rate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
